{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. GRAD SCHOOL ADMISSIONS: EVALUATING BINARY CLASSIFIERS\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introducing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(515, 3) (129, 3)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.set_option(\"display.max_columns\", 99)\n",
    "pd.set_option(\"display.max_rows\", 999)\n",
    "pd.set_option('precision', 3)\n",
    "\n",
    "admissions = pd.read_csv('admissions.csv')\n",
    "train, test = train_test_split(admissions, test_size=0.2, random_state=42)\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    403\n",
       "1    112\n",
       "Name: predicted_label, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(train[[\"gpa\"]], train[\"admit\"])\n",
    "\n",
    "labels = model.predict(train[[\"gpa\"]])\n",
    "train_ = train.copy()\n",
    "train_[\"predicted_label\"] = labels\n",
    "train_[\"predicted_label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admit</th>\n",
       "      <th>gpa</th>\n",
       "      <th>gre</th>\n",
       "      <th>predicted_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>1</td>\n",
       "      <td>3.606</td>\n",
       "      <td>570.618</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>2.783</td>\n",
       "      <td>585.700</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>0</td>\n",
       "      <td>3.227</td>\n",
       "      <td>450.289</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>1</td>\n",
       "      <td>3.540</td>\n",
       "      <td>615.932</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>1</td>\n",
       "      <td>3.743</td>\n",
       "      <td>593.282</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     admit    gpa      gre  predicted_label\n",
       "515      1  3.606  570.618                1\n",
       "29       0  2.783  585.700                0\n",
       "274      0  3.227  450.289                0\n",
       "576      1  3.540  615.932                1\n",
       "434      1  3.743  593.282                1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Accuracy\n",
    "\n",
    "$$\n",
    "Accuracy = \\frac{n Correctly-Predicted}{n Observations}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>admit</th>\n",
       "      <th>gpa</th>\n",
       "      <th>gre</th>\n",
       "      <th>predicted_label</th>\n",
       "      <th>actual_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>515</th>\n",
       "      <td>1</td>\n",
       "      <td>3.606</td>\n",
       "      <td>570.618</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>2.783</td>\n",
       "      <td>585.700</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>0</td>\n",
       "      <td>3.227</td>\n",
       "      <td>450.289</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>1</td>\n",
       "      <td>3.540</td>\n",
       "      <td>615.932</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>434</th>\n",
       "      <td>1</td>\n",
       "      <td>3.743</td>\n",
       "      <td>593.282</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     admit    gpa      gre  predicted_label  actual_label\n",
       "515      1  3.606  570.618                1             1\n",
       "29       0  2.783  585.700                0             0\n",
       "274      0  3.227  450.289                0             0\n",
       "576      1  3.540  615.932                1             1\n",
       "434      1  3.743  593.282                1             1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_[\"actual_label\"] = train_[\"admit\"]\n",
    "matches = train_[\"predicted_label\"] == train_[\"actual_label\"]\n",
    "correct_predictions = train_[matches]\n",
    "correct_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5388198757763976\n"
     ]
    }
   ],
   "source": [
    "accuracy = len(correct_predictions) / len(admissions)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Binary classification outcomes\n",
    "- `True Positive`: the model predicted that the label would be `Positive`, and that ended up being `True`\n",
    "- `True Negative`: the model predicted that the label would be `Negative`, and that ended up being `True`\n",
    "- `False Positive`: the model predicted that the label would be `Positive`, but that was `False` (the actual label was False)\n",
    "- `False Negative`: the model predicted that the label would be `Negative`, but that was `False` (the actual value was True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73\n",
      "274\n"
     ]
    }
   ],
   "source": [
    "TP_filter = (train_[\"predicted_label\"] == 1) & (train_[\"actual_label\"] == 1)\n",
    "TP = len(train_[TP_filter])\n",
    "\n",
    "TN_filter = (train_[\"predicted_label\"] == 0) & (train_[\"actual_label\"] == 0)\n",
    "TN = len(train_[TN_filter])\n",
    "\n",
    "print(TP)\n",
    "print(TN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sensitivity\n",
    "- `True Positive Rate` - The proportion of applicants that were correctly admitted:\n",
    "$$\n",
    "TPR = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "- How effective is this model at identifying positive outcomes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3613861386138614\n"
     ]
    }
   ],
   "source": [
    "TP_filter = (train_[\"predicted_label\"] == 1) & (train_[\"actual_label\"] == 1)\n",
    "TP = len(train_[TP_filter])\n",
    "\n",
    "FN_filter = (train_[\"predicted_label\"] == 0) & (train_[\"actual_label\"] == 1)\n",
    "FN = len(train_[FN_filter])\n",
    "\n",
    "sensitivity = TP / (TP + FN)\n",
    "print(sensitivity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the sensitivity of the model is around 36.1% and only about 1 in 8 students that should have been admitted were actually admitted. In the context of predicting student admissions, this probably isn't too bad of a thing. Graduate schools can only admit a select number of students into their programs and by definition they end up rejecting many qualified students that would have succeeded.\n",
    "\n",
    "In the healthcare context, however, low sensitivity could mean a severe loss of life. If a classification model is only catching 36.1% of positive cases for an illness, then around 7 of 8 people are going undiagnosed (being classified as false negatives).\n",
    "## 5. Specificity\n",
    "- `True Negative Rate` - The proportion of applicants that were correctly rejected:\n",
    "$$\n",
    "TPR = \\frac{TP}{TP + FN}\n",
    "$$\n",
    "- How effective is this model at identifying negative outcomes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8753993610223643\n"
     ]
    }
   ],
   "source": [
    "TN_filter = (train_[\"predicted_label\"] == 0) & (train_[\"actual_label\"] == 0)\n",
    "TN = len(train_[TN_filter])\n",
    "\n",
    "FP_filter = (train_[\"predicted_label\"] == 1) & (train_[\"actual_label\"] == 0)\n",
    "FP = len(train_[FP_filter])\n",
    "\n",
    "specificity = TN / (FP + TN)\n",
    "print(specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the specificity of the model is 87.5%. This means that the model is really good at knowing which applicants to reject. Since a small number of applicants were accepted that applied, it's important that the model reject people correctly who wouldn't have otherwise been accepted."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
